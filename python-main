import os
import re
import sys
import torch
import subprocess
import numpy as np
from google.colab import drive
from unsloth import FastLanguageModel
from sentence_transformers import SentenceTransformer, util

# ==============================================================================
# 1. RAG SÄ°STEMÄ° KURULUMU (BÄ°LGÄ° BANKASI)
# ==============================================================================
print("ULTIMATE KOD DOKTORU (RAM + RAG) BAÅžLATILIYOR...")

if not os.path.exists('/content/drive'):
    drive.mount('/content/drive')

# Embedding Modeli (Metinleri sayÄ±ya Ã§eviren beyin)
print("RAG Modeli YÃ¼kleniyor (all-MiniLM-L6-v2)...")
embedder = SentenceTransformer('all-MiniLM-L6-v2')

# SÄ°MÃœLASYON VERÄ°SÄ° (Senin Agent.pdf Ä°Ã§eriÄŸi) [cite: 1-174]
# Normalde burasÄ± PDF'ten okunur ama hÄ±zlÄ± Ã§alÄ±ÅŸmasÄ± iÃ§in chunk'larÄ± buraya gÃ¶mdÃ¼m.
knowledge_base = [
    "ReAct (Reasoning + Acting), BÃ¼yÃ¼k Dil Modellerinin (LLM) problem Ã§Ã¶zen otonom ajanlara dÃ¶nÃ¼ÅŸmesini saÄŸlayan bir mimaridir.",
    "ReAct dÃ¶ngÃ¼sÃ¼ Ã¼Ã§ adÄ±mdan oluÅŸur: Thought (DÃ¼ÅŸÃ¼nce), Action (Eylem) ve Observation (GÃ¶zlem).",
    "Klasik RAG sistemleri soruyu alÄ±r ve cevabÄ± verir. ReAct mimarisinde ise sistem ham bilgiyi dÃ¶ndÃ¼ren bir Tool olmalÄ±dÄ±r.",
    "Sonsuz DÃ¶ngÃ¼ (Infinite Loop) sorununu Ã§Ã¶zmek iÃ§in 'max_turns' (dÃ¶ngÃ¼ limiti) koyulmalÄ±dÄ±r.",
    "AjanÄ±n elinde olmayan bir aracÄ± uydurmasÄ±na 'HalÃ¼sinasyon' denir. System Prompt'ta araÃ§lar net tanÄ±mlanmalÄ±dÄ±r.",
    "Yol A (RAG): Verilerinizi chunklara bÃ¶lÃ¼n, embeddinglerini Ã§Ä±karÄ±n ve VektÃ¶r VeritabanÄ±na kaydedin.",
    "Yol B (LoRa): Verilerinizle bir Base Modeli fine-tune ederek sektÃ¶rel bilgiye sahip bir LoRa adaptÃ¶rÃ¼ eÄŸitin.",
    "ReAct Agentic RAG'da LLM bir 'OrkestratÃ¶r/Karar Verici' rolÃ¼ndedir.",
    "Python executor tool, ajanÄ±n yazdÄ±ÄŸÄ± Python kodunu sanal ortamda Ã§alÄ±ÅŸtÄ±rÄ±r ve sonucunu Observation olarak dÃ¶ner.",
]

# Verileri VektÃ¶re Ã‡evir (Embedding)
print("Bilgi BankasÄ± Ä°ndeksleniyor...")
corpus_embeddings = embedder.encode(knowledge_base, convert_to_tensor=True)

# ==============================================================================
# 2. LOCAL MODEL (DEEP-100) YÃœKLEME
# ==============================================================================
CHECKPOINT_PATH = "/content/drive/MyDrive/CodeGen_Project/models/deep_instruction/checkpoints/checkpoint-100"

try:
    if model: pass
except:
    print(f"\nAna Beyin YÃ¼kleniyor: {CHECKPOINT_PATH}")
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=CHECKPOINT_PATH,
        max_seq_length=2048,
        dtype=None,
        load_in_4bit=True,
    )
    FastLanguageModel.for_inference(model)

# ==============================================================================
# 3. ARAÃ‡LAR (TOOLS) - ÅžÄ°MDÄ° RAG ARACI DA VAR!
# ==============================================================================

def python_executor_tool(code_snippet):
    """Kodu Ã§alÄ±ÅŸtÄ±rÄ±r."""
    print(f"\n[ACTION] Kod Ã‡alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
    try:
        code_snippet = code_snippet.replace("```python", "").replace("```", "").strip()
        result = subprocess.run(
            [sys.executable, "-c", code_snippet],
            capture_output=True, text=True, timeout=5
        )
        if result.stderr:
            return f"HATA: {result.stderr.strip()}"
        return f"Ã‡IKTI: {result.stdout.strip()}" if result.stdout else "Ã‡IKTI: (BoÅŸ)"
    except Exception as e:
        return f"SÄ°STEM HATASI: {str(e)}"

def rag_retrieval_tool(query):
    """Bilgi BankasÄ±nda (VektÃ¶r DB) arama yapar."""
    print(f"\n[ACTION] Bilgi BankasÄ±nda AranÄ±yor: '{query}'")

    # Soruyu vektÃ¶re Ã§evir
    query_embedding = embedder.encode(query, convert_to_tensor=True)

    # En yakÄ±n benzerliÄŸi bul (Cosine Similarity)
    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]
    top_results = torch.topk(cos_scores, k=2) # En iyi 2 sonucu getir

    results = []
    print(f"   Bulunan DÃ¶kÃ¼manlar:")
    for score, idx in zip(top_results[0], top_results[1]):
        doc = knowledge_base[idx]
        print(f"   - (Skor: {score:.4f}) {doc}")
        results.append(doc)

    return "\n".join(results)

# ==============================================================================
# 5. GÃœNCELLENMÄ°Åž ANA DÃ–NGÃœ (SMART REGEX - FORMAT TAKINTISI YOK)
# ==============================================================================
def ultimate_agent_session():
    print("\n" + "="*70)
    print("ULTIMATE AJAN V2 (AkÄ±llÄ± Kod AlgÄ±lama)")
    print("="*70)

    system_prompt = """Sen 'Kod Doktoru'sun.
    MEVCUT ARAÃ‡LARIN:
    1. [python_executor]: Kod yazÄ±p Ã§alÄ±ÅŸtÄ±rmak iÃ§in.
    2. [rag_retrieval]: Bilgi aramak iÃ§in.

    KURALLAR:
    - Kod yazarken "sys", "argv", "input" kullanma.
    - HafÄ±zanÄ± kullan.
    """

    chat_history = [{"role": "system", "content": system_prompt}]

    while True:
        user_input = input("\nASK/CODE:\n>> ")
        if user_input.lower() in ['exit', 'q', 'Ã§Ä±k']:
            print("GÃ¶rÃ¼ÅŸÃ¼rÃ¼z!")
            break

        print("\nAjan dÃ¼ÅŸÃ¼nÃ¼yor...")
        chat_history.append({"role": "user", "content": user_input})

        if len(chat_history) > 10: chat_history = [chat_history[0]] + chat_history[-6:]

        # --- DÃœÅžÃœNME ---
        text_input = tokenizer.apply_chat_template(chat_history, tokenize=False, add_generation_prompt=True)
        inputs = tokenizer([text_input], return_tensors="pt").to("cuda")

        outputs = model.generate(
            **inputs,
            max_new_tokens=1024,
            stop_strings=["User:"],
            tokenizer=tokenizer,
            temperature=0.1,
            do_sample=True
        )

        full_response = tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]
        response = full_response.strip()

        print(f"\nTHOUGHT:\n{response}")
        chat_history.append({"role": "assistant", "content": response})

        # --- EYLEM (AKILLI REGEX BURADA!) ---

        # 1. RAG KontrolÃ¼
        if "rag_retrieval" in response:
            obs = rag_retrieval_tool(user_input)
            chat_history.append({"role": "user", "content": f"Observation: {obs}"})
            print(f"ðŸ‘€ RAG SONUCU: Bilgi bulundu.")

        # 2. Kod KontrolÃ¼ (GÃœNCELLENDÄ°)
        elif "python_executor" in response or "def " in response or "print(" in response:

            # Ã–nce Markdown kutucuÄŸu ara
            code_match = re.search(r"```python(.*?)```", response, re.DOTALL)

            if code_match:
                code_to_run = code_match.group(1).strip()
            else:
                # Markdown YOKSA ama iÃ§inde kod varsa (Fallback MekanizmasÄ±)
                # BasitÃ§e tÃ¼m yanÄ±tÄ± kod olarak al (ama gereksiz metinleri temizle)
                print("UyarÄ±: Markdown bulunamadÄ±, dÃ¼z metin kod olarak deneniyor...")
                code_to_run = response.replace("python_executor", "").strip()

            if "sys.argv" in code_to_run or "input(" in code_to_run:
                 print("Kod gÃ¼venli deÄŸil (sys/input), Ã§alÄ±ÅŸtÄ±rÄ±lmadÄ±.")
            else:
                obs = python_executor_tool(code_to_run)
                print(f"{obs}")
                chat_history.append({"role": "user", "content": f"Observation: {obs}"})

# Ã‡alÄ±ÅŸtÄ±r
ultimate_agent_session()
